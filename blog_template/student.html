<h2>Methodology – Student Model Distillation</h2>

<p>
To compress the behavior of our pattern-specialized teachers into a more deployable model, we train a compact student LLM via supervised knowledge distillation. The student’s goal is to mimic the teachers’ structured outputs on serialized transaction subgraphs, so that at inference time we can rely on a single lightweight model instead of repeatedly querying large external LLMs. We use a Tinker-managed fine-tuning workflow, which allows us to fine-tune gated Llama models without downloading them locally. In all experiments the student is an autoregressive decoder-only model, instantiated as either a 1B-parameter or 3B-parameter Llama-3.2 variant. Unless otherwise noted, we use a batch size of 16, learning rate of 1e-4, and a single epoch; Tinker logs training and validation negative log-likelihood (NLL) as our primary optimization signal.
</p>

<p>
The distillation dataset is derived from the teacher-labeled k=2 neighborhoods described above. Each example consists of a transaction subgraph centered on a candidate transaction, serialized into text with node types, edge directions, and basic transaction metadata. For each subgraph, the supervising teacher (fan-in or stack specialist) produces a structured JSON judgment containing a binary suspiciousness label, a pattern label, and a short rationale. We convert each labeled example into a supervised fine-tuning conversation with three messages: a system prompt describing the AML classification task and required output format, a user message containing the serialized k-hop neighborhood and instructions, and an assistant message containing the teacher’s JSON response. This format encourages the student to learn both the categorical decisions (suspicious vs. not suspicious, pattern label) and the teacher’s explanation style. The resulting dataset is randomly split into train, validation, and test subsets, with teacher outputs treated as ground truth for the student.
</p>

<p>
We evaluate the student along two complementary axes. First, we track validation mean NLL over teacher outputs as a loss-based measure of how well the student models the teacher distribution: lower NLL indicates that the student assigns higher probability to the teacher’s token sequences. Second, on the held-out test set we parse the student’s generated JSON and compare the predicted pattern label against the teacher’s label. For each typology (fan-in, stack), we compute precision and recall along with approximate 95% confidence intervals obtained via bootstrapped resampling. These pattern-level metrics provide a more interpretable view of how well the student replicates the teachers’ pattern decisions, beyond raw loss.
</p>

<p>
We run two main experiments on the student model. <em>Experiment 1 (student model size)</em> trains two students on the same full teacher dataset: a 1B and a 3B Llama-3.2 model, with all other hyperparameters held fixed. We compare their performance using validation mean NLL and pattern-level metrics. <em>Experiment 2 (teacher dataset size)</em> holds the student architecture fixed (1B model) and varies the fraction of teacher-labeled data used during training: 10%, 25%, 50%, and 100% of available examples. For each fraction we train a separate student from scratch and evaluate validation mean NLL. In both experiments, pattern precision and recall are computed on a shared held-out test set that is not used for training in any configuration.
</p>

<h2>Discussion – Student Performance</h2>

<p>
Overall, the student model learns to approximate the teachers’ structured judgments with reasonable fidelity despite being substantially smaller and cheaper to run. In qualitative inspection, the student’s rationales closely follow the teachers’ narrative style, referencing similar entities and transaction motifs (for example, clusters of inflows or long outgoing chains), which suggests that the model has internalized more than just the final labels. Pattern-level metrics summarize this behavior more concretely: on the held-out test set, the student achieves moderate precision for fan-in but higher precision for stack, while recall is higher for fan-in and moderately high for stack. Error bars are wide due to limited sample size, but the trends are consistent with the teacher results, indicating that the student has learned the teachers’ relative operating points rather than collapsing to a trivial baseline.
</p>
<h3>Experiment 1: Effect of Student Model Size</h3>
<img src="./images/student_experiment1.png" width=512px/>
<p>
Experiment 1 shows a clear benefit from increased capacity. The 3B student achieves a lower validation mean NLL (roughly 1.98) than the 1B student (roughly 2.25) when both are trained on the full teacher dataset. This implies that the larger model is better at modeling the distribution of teacher outputs and is less surprised by the teachers’ token sequences. Because the outputs combine discrete labels with free-form rationales, capturing this mixture requires sufficient capacity to represent both the pattern boundaries and the stylistic variability of explanations. The 3B student appears better able to track these nuances, suggesting that modest increases in model size can yield meaningful gains in reasoning fidelity without resorting to very large models.
</p>
<h3>Experiment 2: Effect of Teacher Dataset Size</h3>
<img src="./images/student_experiment2.png" width=512px/>
<p>
From experiment 2, we learn that as we increase the fraction of teacher-labeled data used for training a fixed 1B student, validation NLL decreases monotonically: from about 3.1 at 10% of the data to around 2.2 when trained on 100% of the examples. The largest gains occur between 10% and 50%, with diminishing but still positive improvements from 50% to 100%. With only a small subset of examples, the student underfits rare subgraph patterns and explanation styles, resulting in higher loss. As the dataset grows, the model encounters more variations of fan-in and stack neighborhoods and more diverse rationales, allowing it to better approximate the teacher distribution. The fact that NLL continues to improve up to 100% suggests that, given more labeled subgraphs or additional typologies, the student could likely benefit from even larger distillation corpora.
</p>
<h3>Pattern-Level Student Evaluation</h3>
<img src="./images/student_pr.png" width=512px/>
<p>
The pattern-wise precision–recall plot for the student mirrors many of the qualitative trends seen in the teacher evaluation. For fan-in, the student achieves high recall but relatively low precision, meaning it successfully captures most of the cases the teachers identify as fan-in, but at the cost of extra false positives. For stack, the student trades some recall for higher precision, behaving more conservatively. The overlap between student and teacher confidence intervals indicates that, within statistical noise, the student is broadly aligned with the teachers’ operating characteristics. From an AML perspective, this trade-off can be useful: high-recall fan-in detection is valuable as a front-end filter for further investigation, while a more precise stack detector reduces analyst burden on rarer, structurally complex typologies. Taken together, the experiments suggest that a compact LLM can absorb a meaningful fraction of the teachers’ reasoning behavior and that both model capacity and the amount of teacher supervision significantly affect distillation quality.
</p>
